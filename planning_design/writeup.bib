
@book{munzner:2014,
  title = {Visualization {{Analysis}} and {{Design}}},
  author = {Munzner, Tamara},
  year = {2014},
  publisher = {{CRC Press}},
  isbn = {978-1-4665-0891-0},
  series = {{{AK Peters Visualization Series}}}
}

@article{wood:2018,
  title = {Design {{Exposition}} with {{Literate Visualization}}},
  author = {Wood, J. and Kachkaev, A. and Dykes, J.},
  year = {2018},
  month = aug,
  volume = {25},
  issn = {1941-0506},
  doi = {10.1109/TVCG.2018.2864836},
  abstract = {We propose a new approach to the visualization design and communication process, literate visualization, based upon and extending, Donald Knuth's idea of literate programming. It integrates the process of writing data visualization code with description of the design choices that led to the implementation (design exposition). We develop a model of design exposition characterised by four visualization designer architypes: the evaluator, the autonomist, the didacticist and the rationalist. The model is used to justify the key characteristics of literate visualization: `notebook' documents that integrate live coding input, rendered output and textual narrative; low cost of authoring textual narrative; guidelines to encourage structured visualization design and its documentation. We propose narrative schemas for structuring and validating a wide range of visualization design approaches and models, and branching narratives for capturing alternative designs and design views. We describe a new open source literate visualization environment, litvis, based on a declarative interface to Vega and Vega-Lite through the functional programming language Elm combined with markdown for formatted narrative. We informally assess the approach, its implementation and potential by considering three examples spanning a range of design abstractions: new visualization idioms; validation though visualization algebra; and feminist data visualization. We argue that the rich documentation of the design process provided by literate visualization offers the potential to improve the validity of visualization design and so benefit both academic visualization and visualization practice.},
  file = {/home/tom/Zotero/storage/DJMRK5AZ/Wood et al. - 2018 - Design Exposition with Literate Visualization.pdf;/home/tom/Zotero/storage/YGDIPF3U/20081.html},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  language = {en},
  number = {1}
}

@inproceedings{heer:2005,
  title = {Prefuse: A Toolkit for Interactive Information Visualization},
  booktitle = {Proceedings of the {{SIGCHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Heer, Jeffrey and Card, Stuart K. and Landay, James A.},
  year = {2005},
  pages = {421--430},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1054972.1055031},
  isbn = {1-58113-998-5},
  series = {{{CHI}} '05}
}

@book{heller:2015,
  title={The education of a graphic designer},
  author={Heller, Steven},
  year={2015},
  publisher={Simon and Schuster}
}

@article{sadler:2006,
  title={The impact of self-and peer-grading on student learning},
  author={Sadler, Philip M and Good, Eddie},
  journal={Educational assessment},
  volume={11},
  number={1},
  pages={1--31},
  year={2006},
  publisher={Taylor \& Francis}
}

@inproceedings{chen:2013,
author = {Chen, Xi and Bennett, Paul N. and Collins-Thompson, Kevyn and Horvitz, Eric},
title = {Pairwise Ranking Aggregation in a Crowdsourced Setting},
year = {2013},
isbn = {9781450318693},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2433396.2433420},
doi = {10.1145/2433396.2433420},
abstract = {Inferring rankings over elements of a set of objects, such as documents or images, is a key learning problem for such important applications as Web search and recommender systems. Crowdsourcing services provide an inexpensive and efficient means to acquire preferences over objects via labeling by sets of annotators. We propose a new model to predict a gold-standard ranking that hinges on combining pairwise comparisons via crowdsourcing. In contrast to traditional ranking aggregation methods, the approach learns about and folds into consideration the quality of contributions of each annotator. In addition, we minimize the cost of assessment by introducing a generalization of the traditional active learning scenario to jointly select the annotator and pair to assess while taking into account the annotator quality, the uncertainty over ordering of the pair, and the current model uncertainty. We formalize this as an active learning strategy that incorporates an exploration-exploitation tradeoff and implement it using an efficient online Bayesian updating scheme. Using simulated and real-world data, we demonstrate that the active learning strategy achieves significant reductions in labeling cost while maintaining accuracy.},
booktitle = {Proceedings of the Sixth ACM International Conference on Web Search and Data Mining},
pages = {193â€“202},
numpages = {10},
keywords = {pairwise preference, crowdsourcing, ranking},
location = {Rome, Italy},
series = {WSDM '13}
}

